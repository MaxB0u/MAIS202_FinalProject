# -*- coding: utf-8 -*-
"""MAIS202_FinalProject

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VSF5ZL5dUcADf0a2yZIRGwnkxEtacqEB
"""

# we will start by installing then importing the relevant Python libraries
import csv
import random
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn import linear_model
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from google.colab import files
import math
from joblib import dump, load

!pip install -q kaggle

# read in the data as pandas dataframes

#parse it

#linear regression

#gradient descent

#regularization

#output

uploaded = files.upload()
!ls

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!kaggle datasets download -d jpayne/852k-used-car-listings
!ls

!unzip 852k-used-car-listings.zip
!ls

dt_train = 'tc20171021.csv'
with open(dt_train) as csv_file:
  csv_reader = csv.reader(csv_file) 
  data = list(csv_reader)
data.remove(data[0]) #remove labels on top
#want columns 1,2,3,7,8
#7 is make, can forget it
random.shuffle(data)

X_raw2 = list() #year,mileage,make,model
y_raw2 = list() #price

for a in data:
  X_raw2.append([int(a[2]),int(a[3]),a[8],int(a[1]),a[7]])  #removed make
  y_raw2.append(int(a[1]))
print(len(X_raw2))

#vectorize data so every model and make gets mapped to a number
make = dict()
make_counter = dict()
dct2 = {}
X2 = [] #year,mileage,make,model
y2 = [] #price
counter2 = 0
for listing in X_raw2:
  
 # if listing[2] not in model:
  #  model[listing[2]] = counter
   # counter+=1
  
  make_counter[listing[4]] = make_counter.get(listing[4],0) + 1

j = 0  

for i in range(len(X_raw2)):
  if make_counter.get(X_raw2[i][4],0) > 500: 
    if X_raw2[i][4] not in dct2:
      dct2['%s' % X_raw2[i][4]] = []

      dct2.get(X_raw2[i][4]).append(X_raw2[i][0:4])

    else:
      dct2.get(X_raw2[i][4]).append(X_raw2[i][0:4])


    #X[j][2] = model[X_raw[i][2]]   #this line changes model names to number in X and X_raw...

    #dct['%s' % X_raw[i][2]] = []
    j += 1

#now have dict of model:year,mileage,price with each model has at least 20 entries
print(len(X2))
print(len(dct2))
print(dct2['Buick'])
print(len(dct2['Buick']))

a=[]
y = []
for key in dct2:
  
  for i in dct2[key]: 
   if(i[2]+'|'+i[2] not in a):
     a.append(i[2]+"|"+i[2])
   if(i[0] not in y):
     y.append(i[0])
  
  print(key)
  print('Year: '+str(min(y))+' , '+str(max(y)))
  a.sort()
  print(a)
  print()
  y=[]
  a=[]

#change model name to numbers
dctmod = {}
cnt = 0
for entry in dct2:
  for ent in dct2.get(entry):
    if ent[2] not in dctmod:
      dctmod[ent[2]] = cnt
      cnt += 1

for i in dct2:
  for j in dct2.get(i):
    j[2] = dctmod[j[2]]

def train_test_valid_split_dict(entries, train_size, valid_size):
  train_set = {}
  valid_set = {}
  test_set = {}
  
  for ent in entries:
    length = len(entries[ent])
    train_set[ent] = []
    valid_set[ent] = []
    test_set[ent] = []
    for i in range(length):
      if i<train_size * length:
        train_set.get(ent).append(entries.get(ent)[i])
      elif i<(train_size+valid_size) * length:
        valid_set.get(ent).append(entries.get(ent)[i])
      else:
        test_set.get(ent).append(entries.get(ent)[i])
  
  return train_set,valid_set,test_set

train_set2,valid_set2,test_set2 = train_test_valid_split_dict(dct2, 0.6, 0.2) #split the dict

xx2 = []
yy2 = []

error2 = []
score2 = []
average2 = []
pourc_e2 = []
lst_reg = []


for entry in train_set2:
  name = 'for_reg'+'%s' % entry 
  name = RandomForestRegressor()
  lst_reg.append(name)

  ent = train_set2.get(entry)
  for el in ent:
    xx2.append(el[0:3])
    yy2.append(el[3])
  name.fit(xx2,yy2)

  filename = str(entry)+'.joblib'
  dump(name,filename)


  sc = name.score(xx2,yy2)
  pred = name.predict(xx2)
  e = mean_squared_error(pred,yy2)
  avg = np.mean(yy2)
  pe = math.sqrt(e)/avg
  xx2 = []
  yy2 = []
  error2.append(e)
  score2.append(sc)
  average2.append(avg)
  pourc_e2.append(pe)
   

print(score2)
print(error2)
print(average2)
print(pourc_e2)
print(max(error2))
print(min(error2))

xx2 = []
yy2 = []



entry = 'GMC'
name = 'for_reg'+'%s' % entry 
name = RandomForestRegressor()

ent = train_set2.get(entry)
for el in ent:
  xx2.append(el[0:3])
  yy2.append(el[3])
name.fit(xx2,yy2)

filename = str(entry)+'.joblib'
dump(name,filename)

import os
print( os.getcwd() )
print( os.listdir() )

filename = 'BMW.joblib'
files.download(filename)

f = 'GMC.joblib'
uploaded = drive.CreateFile({'title': f})
uploaded.SetContentFile(f)
uploaded.Upload()
print('Uploaded file with ID {}'.format(uploaded.get('id')))

m = np.mean(error2)
n = np.median(error2)
print(m)
print(n)
print()
print(np.mean(pourc_e2))
print(np.median(pourc_e2))

import pickle
xx2_valid = []
yy2_valid = []

error2_valid = []
score2_valid = []
average2_valid = []
pourc_e2_valid = []
lst_reg = []


for entry in valid_set2:
  name = 'for_reg'+'%s' % entry 
  name = RandomForestRegressor()
  lst_reg.append(name)

  ent = valid_set2.get(entry)
  for el in ent:
    xx2_valid.append(el[0:3])
    yy2_valid.append(el[3])
  name.fit(xx2_valid,yy2_valid)

  sc = name.score(xx2_valid,yy2_valid)
  pred = name.predict(xx2_valid)
  e = mean_squared_error(pred,yy2_valid)
  avg = np.mean(yy2_valid)
  pe = math.sqrt(e)/avg
  xx2_valid = []
  yy2_valid = []
  error2_valid.append(e)
  score2_valid.append(sc)
  average2_valid.append(avg)
  pourc_e2_valid.append(pe)
   

print(score2_valid)
print(error2_valid)
print(average2_valid)
print(pourc_e2_valid)
print(max(error2_valid))
print(min(error2_valid))

xx2_test = []
yy2_test = []

error2_test = []
score2_test = []
average2_test = []
pourc_e2_test = []
lst_reg = []


for entry in test_set2:
  name = 'for_reg'+'%s' % entry 
  name = RandomForestRegressor()
  lst_reg.append(name)

  ent = test_set2.get(entry)
  for el in ent:
    xx2_test.append(el[0:3])
    yy2_test.append(el[3])
  name.fit(xx2_test,yy2_test)
  sc = name.score(xx2_test,yy2_test)
  pred = name.predict(xx2_test)
  e = mean_squared_error(pred,yy2_test)
  avg = np.mean(yy2_test)
  pe = math.sqrt(e)/avg
  xx2_test = []
  yy2_test = []
  error2_test.append(e)
  score2_test.append(sc)
  average2_test.append(avg)
  pourc_e2_test.append(pe)
   

print(score2_test)
print(error2_test)
print(average2_test)
print(pourc_e2_test)
print(max(error2_test))
print(min(error2_test))

a=[]
for entry in test_set2:
  a.append(entry)
print(a)

m = np.mean(error2_valid)
n = np.median(error2_valid)
print(m)
print(n)
print()
print(np.mean(pourc_e2_valid))
print(np.median(pourc_e2_valid))
print()
print()
m = np.mean(error2_test)
n = np.median(error2_test)
print(m)
print(n)
print()
print(np.mean(pourc_e2_test))
print(np.median(pourc_e2_test))

plt.plot(error2)
plt.xlabel('List of models')
plt.ylabel('MSE')
plt.title("List of errors per model on the validation set")
plt.show()

plt.plot(pourc_e2)
plt.xlabel('List of models')
plt.ylabel('error (%)')
plt.title("List of errors per model on the training set")
plt.show()

plt.plot(error2_valid)
plt.xlabel('List of models')
plt.ylabel('MSE')
plt.title("List of errors per model on the validation set")
plt.show()

plt.plot(pourc_e2_valid)
plt.xlabel('List of models')
plt.ylabel('error (%)')
plt.title("List of errors per model on the training set")
plt.show()

plt.plot(error2_test)
plt.xlabel('List of models')
plt.ylabel('MSE')
plt.title("List of errors per model on the validation set")
plt.show()

plt.plot(pourc_e2_test)
plt.xlabel('List of models')
plt.ylabel('error (%)')
plt.title("List of errors per model on the training set")
plt.show()

print(dctmod)
a=[]
for i in dctmod:
  a.append(i)
print(a)

a=dct2['Buick'][0][0:3]
print(a)
name.predict([a])