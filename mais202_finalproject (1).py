# -*- coding: utf-8 -*-
"""MAIS202_FinalProject

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VSF5ZL5dUcADf0a2yZIRGwnkxEtacqEB
"""

# we will start by installing then importing the relevant Python libraries
import csv
import random
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn import linear_model
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from google.colab import files

!pip install -q kaggle

# read in the data as pandas dataframes

#parse it

#linear regression

#gradient descent

#regularization

#output

uploaded = files.upload()
!ls

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!kaggle datasets download -d jpayne/852k-used-car-listings
!ls

!unzip 852k-used-car-listings.zip
!ls

dataset_filename = 'BUICK_preliminary result.csv'
dt_train = 'tc20171021.csv'
dt_test = 'true_car_listings.csv'

with open(dt_train) as csv_file:
  csv_reader = csv.reader(csv_file) 
  data = list(csv_reader)
data.remove(data[0]) #remove labels on top
#want columns 1,2,3,7,8
#7 is make, can forget it

random.shuffle(data)
X_raw = list() #year,mileage,make,model
y_raw = list() #price

for a in data:
  X_raw.append([int(a[2]),int(a[3]),int(a[1]),a[8]])  #removed make
  y_raw.append(int(a[1]))
print(len(X_raw))

#vectorize data so every model and make gets mapped to a number
model = dict()
model_counter = dict()
dct = {}
X = [] #year,mileage,make,model
y = [] #price
counter = 0
for listing in X_raw:
  
 # if listing[2] not in model:
  #  model[listing[2]] = counter
   # counter+=1
  
  model_counter[listing[3]] = model_counter.get(listing[3],0) + 1

j = 0  

for i in range(len(X_raw)):
  if model_counter.get(X_raw[i][3],0) > 500: 
    if X_raw[i][3] not in dct:
      dct['%s' % X_raw[i][3]] = []

      dct.get(X_raw[i][3]).append(X_raw[i][0:3])

    else:
      dct.get(X_raw[i][3]).append(X_raw[i][0:3])

    X.append(X_raw[i])
    y.append(y_raw[i])

    #X[j][2] = model[X_raw[i][2]]   #this line changes model names to number in X and X_raw...

    #dct['%s' % X_raw[i][2]] = []
    j += 1

#now have dict of model:year,mileage,price with each model has at least 20 entries
print(len(X))
print(len(dct))
print(dct['EncoreFWD'])
print(len(dct['EncoreFWD']))

X_raw2 = list() #year,mileage,make,model
y_raw2 = list() #price

for a in data:
  X_raw2.append([int(a[2]),int(a[3]),a[8],int(a[1]),a[7]])  #removed make
  y_raw2.append(int(a[1]))
print(len(X_raw2))

#vectorize data so every model and make gets mapped to a number
make = dict()
make_counter = dict()
dct2 = {}
X2 = [] #year,mileage,make,model
y2 = [] #price
counter2 = 0
for listing in X_raw2:
  
 # if listing[2] not in model:
  #  model[listing[2]] = counter
   # counter+=1
  
  make_counter[listing[4]] = make_counter.get(listing[4],0) + 1

j = 0  

for i in range(len(X_raw2)):
  if make_counter.get(X_raw2[i][4],0) > 500: 
    if X_raw2[i][4] not in dct2:
      dct2['%s' % X_raw2[i][4]] = []

      dct2.get(X_raw2[i][4]).append(X_raw2[i][0:4])

    else:
      dct2.get(X_raw2[i][4]).append(X_raw2[i][0:4])

    X.append(X_raw2[i])
    y.append(y_raw2[i])

    #X[j][2] = model[X_raw[i][2]]   #this line changes model names to number in X and X_raw...

    #dct['%s' % X_raw[i][2]] = []
    j += 1

#now have dict of model:year,mileage,price with each model has at least 20 entries
print(len(X2))
print(len(dct2))
print(dct2['Buick'])
print(len(dct2['Buick']))

#change model name to numbers
dctmod = {}
cnt = 0
for entry in dct2:
  for ent in dct2.get(entry):
    if ent[2] not in dctmod:
      dctmod[ent[2]] = cnt
      cnt += 1

for i in dct2:
  for j in dct2.get(i):
    j[2] = dctmod[j[2]]

def train_test_valid_split_dict(entries, train_size, valid_size):
  train_set = {}
  valid_set = {}
  test_set = {}
  
  for ent in entries:
    length = len(entries[ent])
    train_set[ent] = []
    valid_set[ent] = []
    test_set[ent] = []
    for i in range(length):
      if i<train_size * length:
        train_set.get(ent).append(entries.get(ent)[i])
      elif i<(train_size+valid_size) * length:
        valid_set.get(ent).append(entries.get(ent)[i])
      else:
        test_set.get(ent).append(entries.get(ent)[i])
  
  return train_set,valid_set,test_set

def train_test_valid_split(X, y, train_size, valid_size):
  ### Answer starts here ###
  X_train = []
  X_test = []
  X_valid = []
  y_valid = []
  y_train = []
  y_test = []
  split_valid = len(X)*(train_size + valid_size)
  split_value = len(X)*train_size
  for i in range(0,len(X)):
    if i<split_value:
      X_train.append(X[i])
      y_train.append(y[i])
    elif i<split_valid:
      X_valid.append(X[i])
      y_valid.append(y[i])
    else:
      X_test.append(X[i])
      y_test.append(y[i])
  
  return X_train, X_valid, X_test, y_train, y_valid, y_test
  ### Answer ends here ###

X_train, X_valid, X_test, y_train, y_valid, y_test = train_test_valid_split(X, y, 0.6, 0.2) #split between training and tst data
train_set,valid_set,test_set = train_test_valid_split_dict(dct, 0.6, 0.2) #split the dict
print(len(train_set))
print(len(valid_set))
print(len(test_set)) #should all be the same since all sets should have all the models

train_set2,valid_set2,test_set2 = train_test_valid_split_dict(dct2, 0.6, 0.2) #split the dict

print(len(dct))
print(train_set)

#support vector regression
#svr_reg = SVR()
#svr_reg.fit(X_train,y_train)
#y_pred_train = svr_reg.predict(X_train)
#print(mean_squared_error(svr_reg.predict(X_train),y_train))
#print(mean_squared_error(svr_reg.predict(X_test),y_test))
#print(svr_clf.score(X_train, y_train))
#print(svr_clf.score(X_test, y_test))
import math
#random forest regression on first model using sickit learning
xx = []
yy = []

error = []
score = []
average = []
pourc_e = []
lst_reg = []


for entry in train_set:
  name = 'for_reg'+'%s' % entry 
  name = RandomForestRegressor()
  lst_reg.append(name)

  ent = train_set.get(entry)
  for el in ent:
    xx.append(el[0:2])
    yy.append(el[2])
  name.fit(xx,yy)
  sc = name.score(xx,yy)
  pred = name.predict(xx)
  e = mean_squared_error(pred,yy)
  avg = np.mean(yy)
  pe = math.sqrt(e)/avg
  xx = []
  yy = []
  error.append(e)
  score.append(sc)
  average.append(avg)
  pourc_e.append(pe)
   

print(score)
print(error)
print(average)
print(pourc_e)
print(max(error))
print(min(error))



m = np.mean(error)
n = np.median(error)
print(m)
print(n)
print()
print(np.mean(pourc_e))
print(np.median(pourc_e))

valid_xx = []
valid_yy = []

valid_error = []
valid_score = []
valid_average = []
valid_pourc_e = []

i = 0
for entry in valid_set:
  name = lst_reg[i]
  i+=1

  ent = valid_set.get(entry)
  for el in ent:
    valid_xx.append(el[0:2])
    valid_yy.append(el[2])
  sc = name.score(valid_xx,valid_yy)
  pred = name.predict(valid_xx)
  e = mean_squared_error(pred,valid_yy)
  avg = np.mean(valid_yy)
  pe = math.sqrt(e)/avg
  valid_xx = []
  valid_yy = []

  valid_error.append(e)
  valid_score.append(sc)
  valid_average.append(avg)
  valid_pourc_e.append(pe)
   

print(valid_score)
print(valid_error)
print(valid_average)
print(valid_pourc_e)
print(max(valid_error))
print(min(valid_error))

test_xx = []
test_yy = []

test_error = []
test_score = []
test_average = []
test_pourc_e = []

i = 0
for entry in test_set:
  name = lst_reg[i]
  i+=1

  ent = test_set.get(entry)
  for el in ent:
    test_xx.append(el[0:2])
    test_yy.append(el[2])
  sc = name.score(test_xx,test_yy)
  pred = name.predict(test_xx)
  e = mean_squared_error(pred,test_yy)
  avg = np.mean(test_yy)
  pe = math.sqrt(e)/avg
  test_xx = []
  test_yy = []

  test_error.append(e)
  test_score.append(sc)
  test_average.append(avg)
  test_pourc_e.append(pe)
   

print(test_score)
print(test_error)
print(test_average)
print(test_pourc_e)
print(max(test_error))
print(min(test_error))

m = np.mean(valid_error)
n = np.median(valid_error)
print(m)
print(n)
print()
print(np.mean(valid_pourc_e))
print(np.median(valid_pourc_e))
print()
m = np.mean(test_error)
n = np.median(test_error)
print(m)
print(n)
print()
print(np.mean(test_pourc_e))
print(np.median(test_pourc_e))

#random forest regression using sickit learning
#for_reg = RandomForestRegressor()
#for_reg.fit(X_train,y_train)

#print(for_reg.score(X_train, y_train))
#print(mean_squared_error(for_reg.predict(X_train),y_train))

#pred_valid = for_reg.predict(X_valid) #use validation set
#print(for_reg.score(X_valid, y_valid))
#print(mean_squared_error(pred_valid,y_valid))

sums = 0
for a in data:
  sums+=int(a[1])
print(sums/len(data))

"23590371.079348955"
print(math.sqrt(23590371)/(sums/len(data)))

pred_test = for_reg.predict(X_test) #use testing data
print(for_reg.score(X_test, y_test))
print(mean_squared_error(pred_test,y_test))

#instead, lets try to separate the list by model and do a regression per model instead
#To do this, we need to further parse the data. And, before using the data, we will need to add
#a check to make sure there are enough sample to d a regression on that model
#if there is not, that model will be taken out from the list.

for mod in model.keys():
  mod = list()
#Start from X
i = 0
for x in X:
  X[3].append([X[0],X[1],y[i]])
  i+=1

print

def svr_regression(model,name):
  X = list()
  y = list()
  for mod in model:
    X.append([mod[0],mod[1]])
    y.append(mod[2])
  X_train, X_test, y_train, y_test = train_test_split(X, y, 0.8) #split between training and tst data   
  svr_reg.fit(X_train,y_train)

  print("------------------"+name+"-----------------")
  print(mean_squared_error(svr_reg.predict(X_train),y_train))
  print(mean_squared_error(svr_reg.predict(X_test),y_test))
  print(svr_clf.score(X_train, y_train))
  print(svr_clf.score(X_test, y_test))
  print("--------------------------------------------")

plt.plot(error)
plt.xlabel('List of models')
plt.ylabel('MSE')
plt.title("List of errors per model on the training set")
plt.show()

plt.plot(pourc_e)
plt.xlabel('List of models')
plt.ylabel('error (%)')
plt.title("List of errors per model on the training set")
plt.show()

plt.plot(valid_error)
plt.xlabel('List of models')
plt.ylabel('MSE')
plt.title("List of errors per model on the validation set")
plt.show()

plt.plot(valid_pourc_e)
plt.xlabel('List of models')
plt.ylabel('error (%)')
plt.title("List of errors per model on the training set")
plt.show()

plt.plot(test_error)
plt.xlabel('List of models')
plt.ylabel('MSE')
plt.title("List of errors per model on the validation set")
plt.show()

plt.plot(test_pourc_e)
plt.xlabel('List of models')
plt.ylabel('error (%)')
plt.title("List of errors per model on the training set")
plt.show()

xx2 = []
yy2 = []

error2 = []
score2 = []
average2 = []
pourc_e2 = []
lst_reg = []


for entry in train_set2:
  name = 'for_reg'+'%s' % entry 
  name = RandomForestRegressor()
  lst_reg.append(name)

  ent = train_set2.get(entry)
  for el in ent:
    xx2.append(el[0:3])
    yy2.append(el[3])
  name.fit(xx2,yy2)
  sc = name.score(xx2,yy2)
  pred = name.predict(xx2)
  e = mean_squared_error(pred,yy2)
  avg = np.mean(yy2)
  pe = math.sqrt(e)/avg
  xx2 = []
  yy2 = []
  error2.append(e)
  score2.append(sc)
  average2.append(avg)
  pourc_e2.append(pe)
   

print(score2)
print(error2)
print(average2)
print(pourc_e2)
print(max(error2))
print(min(error2))

m = np.mean(error2)
n = np.median(error2)
print(m)
print(n)
print()
print(np.mean(pourc_e2))
print(np.median(pourc_e2))

xx2_valid = []
yy2_valid = []

error2_valid = []
score2_valid = []
average2_valid = []
pourc_e2_valid = []
lst_reg = []


for entry in valid_set2:
  name = 'for_reg'+'%s' % entry 
  name = RandomForestRegressor()
  lst_reg.append(name)

  ent = valid_set2.get(entry)
  for el in ent:
    xx2_valid.append(el[0:3])
    yy2_valid.append(el[3])
  name.fit(xx2_valid,yy2_valid)
  sc = name.score(xx2_valid,yy2_valid)
  pred = name.predict(xx2_valid)
  e = mean_squared_error(pred,yy2_valid)
  avg = np.mean(yy2_valid)
  pe = math.sqrt(e)/avg
  xx2_valid = []
  yy2_valid = []
  error2_valid.append(e)
  score2_valid.append(sc)
  average2_valid.append(avg)
  pourc_e2_valid.append(pe)
   

print(score2_valid)
print(error2_valid)
print(average2_valid)
print(pourc_e2_valid)
print(max(error2_valid))
print(min(error2_valid))

xx2_test = []
yy2_test = []

error2_test = []
score2_test = []
average2_test = []
pourc_e2_test = []
lst_reg = []


for entry in test_set2:
  name = 'for_reg'+'%s' % entry 
  name = RandomForestRegressor()
  lst_reg.append(name)

  ent = test_set2.get(entry)
  for el in ent:
    xx2_test.append(el[0:3])
    yy2_test.append(el[3])
  name.fit(xx2_test,yy2_test)
  sc = name.score(xx2_test,yy2_test)
  pred = name.predict(xx2_test)
  e = mean_squared_error(pred,yy2_test)
  avg = np.mean(yy2_test)
  pe = math.sqrt(e)/avg
  xx2_test = []
  yy2_test = []
  error2_test.append(e)
  score2_test.append(sc)
  average2_test.append(avg)
  pourc_e2_test.append(pe)
   

print(score2_test)
print(error2_test)
print(average2_test)
print(pourc_e2_test)
print(max(error2_test))
print(min(error2_test))

m = np.mean(error2_valid)
n = np.median(error2_valid)
print(m)
print(n)
print()
print(np.mean(pourc_e2_valid))
print(np.median(pourc_e2_valid))
print()
print()
m = np.mean(error2_test)
n = np.median(error2_test)
print(m)
print(n)
print()
print(np.mean(pourc_e2_test))
print(np.median(pourc_e2_test))

plt.plot(error2)
plt.xlabel('List of models')
plt.ylabel('MSE')
plt.title("List of errors per model on the validation set")
plt.show()

plt.plot(pourc_e2)
plt.xlabel('List of models')
plt.ylabel('error (%)')
plt.title("List of errors per model on the training set")
plt.show()

plt.plot(error2_valid)
plt.xlabel('List of models')
plt.ylabel('MSE')
plt.title("List of errors per model on the validation set")
plt.show()

plt.plot(pourc_e2_valid)
plt.xlabel('List of models')
plt.ylabel('error (%)')
plt.title("List of errors per model on the training set")
plt.show()

plt.plot(error2_test)
plt.xlabel('List of models')
plt.ylabel('MSE')
plt.title("List of errors per model on the validation set")
plt.show()

plt.plot(pourc_e2_test)
plt.xlabel('List of models')
plt.ylabel('error (%)')
plt.title("List of errors per model on the training set")
plt.show()
